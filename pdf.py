# ==============================================================================
#      PDF ç« ç¯€çµæ§‹åŒ–æ“·å–èˆ‡è¼¸å‡º
# ==============================================================================

# === åŒ¯å…¥å¿…è¦å¥—ä»¶ ===
import os
import re
import json
import cv2
import fitz # PyMuPDF
import time
import camelot
import asyncio
import numpy as np
import pandas as pd
from PIL import Image
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from utility.llm_google import chapter_to_json, extract_ic_model, extract_section
from utility.ingest_queue import ingest_chunks

# GPU åŠŸèƒ½å·²å®Œå…¨ç§»é™¤

# === å¤šåŸ·è¡Œç·’è¨­å®š ===
def get_optimal_workers():
    """
    è‡ªå‹•è¨ˆç®—æœ€ä½³åŸ·è¡Œç·’æ•¸é‡
    """
    cpu_count = os.cpu_count() or 2

    # CPU æ¨¡å¼ï¼šä½¿ç”¨ 75% çš„åŸ·è¡Œç·’æ•¸ï¼Œä¿ç•™ä¸€äº›çµ¦ç³»çµ±
    optimal = max(4, int(cpu_count * 0.75))
    print(f"ğŸ”§ åµæ¸¬åˆ° {cpu_count} å€‹é‚è¼¯è™•ç†å™¨ï¼ˆåŸ·è¡Œç·’ï¼‰")
    print(f"ğŸ’» CPU æ¨¡å¼ï¼šä½¿ç”¨ {optimal} å€‹åŸ·è¡Œç·’ï¼ˆç´„ {optimal/cpu_count*100:.0f}% ä½¿ç”¨ç‡ï¼‰")

    return optimal

# å…¨åŸŸè®Šæ•¸ï¼šç”¨æ–¼è¿½è¹¤å·²å„²å­˜åœ–ç‰‡çš„å…§å®¹ï¼ˆHashå€¼ -> åŸå§‹æª”å/è·¯å¾‘ï¼‰
IMAGE_HASH_CACHE = {}

# ==============================================================================
# === é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ (æ•ˆèƒ½å„ªåŒ–) ===
# ==============================================================================

# ç›®éŒ„è§£æç›¸é—œ
CATALOG_LINE_PATTERN_DOTS = re.compile(
    r"^(.+?)\s*\.{2,}\s*([0-9]+|[IVXLCDMivxlcdm]+|[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬]+)\s*$"
)
CATALOG_LINE_PATTERN_SPACE = re.compile(
    r"^(.+?)\s+([0-9]+|[IVXLCDMivxlcdm]+|[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬]+)\s*$"
)
CATALOG_CHAPTER_PATTERN = re.compile(
    r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )|(\.{3,}\s*\d+)"
)

# åœ–è¡¨æ¨™é¡Œæª¢æ¸¬ (æœ€é«˜é »ï¼Œå½±éŸ¿æœ€å¤§)
FIGURE_TABLE_TITLE_PATTERN = re.compile(
    r"^(åœ–|å›¾|Figure|Fig\.|åœ–ä¾‹|å›¾ä¾‹|è¡¨|Tab\.|Table|å›³|è¡¨|ê·¸ë¦¼)\s*[\.0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+",
    re.IGNORECASE
)

# æ–‡å­—æ¸…ç†ç›¸é—œ
TRAILING_PUNCTUATION_PATTERN = re.compile(
    r'[\s\.]*([\.ï¼Œã€‚ï¼›ã€:ï¼š!ï¼?ï¼Ÿ]+|\.{3,})\s*$'
)
UNSAFE_FILENAME_CHARS_PATTERN = re.compile(r'[\\/:*?"<>|]+')
WHITESPACE_PATTERN = re.compile(r'\s+')

# ç« ç¯€åµæ¸¬ç›¸é—œ
CHAPTER_NUMBER_PREFIX_PATTERN = re.compile(
    r"^\s*[0-9]{1,2}(\.[0-9]{1,2}){0,2}\s*"
)
CHAPTER_PREFIX_PATTERN = re.compile(
    r"^(åœ–|Figure|åœ–ä¾‹|è¡¨|Table|ç« |ç¯€)\s*"
)
CHAPTER_REGEX_STRONG = re.compile(
    r"(ç¬¬\s*[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*ç« )"
)


# ä¿®æ­£æ–¹æ¡ˆï¼šå°‡ MockFiles å®šç¾©åœ¨ try/except ä¹‹å¤–
class MockFiles:
    """ç”¨æ–¼æ¨¡æ“¬é Colab ç’°å¢ƒçš„ files.upload() è¡Œç‚º"""
    def upload(self):
        print("è«‹åœ¨æœ¬åœ°ç’°å¢ƒæ‰‹å‹•å°‡ PDF æª”æ¡ˆæ”¾åœ¨èˆ‡è…³æœ¬ç›¸åŒçš„ç›®éŒ„ï¼Œä¸¦ä¿®æ”¹ pdf_path è®Šæ•¸ã€‚")
        return {"dummy.pdf": None}


# å˜—è©¦åœ¨ Colab ç’°å¢ƒåŒ¯å…¥ filesï¼Œè‹¥å¤±æ•—å‰‡ä½¿ç”¨ MockFiles
try:
    from google.colab import files # type: ignore
    IS_COLAB = True
except ImportError:
    files = MockFiles() # åœ¨é Colab ç’°å¢ƒä¸­ä½¿ç”¨ Mock å¯¦ä¾‹
    IS_COLAB = False

# é è¨­çš„é„°è¿‘è·é›¢ï¼ˆåƒç´ ï¼‰ï¼Œç”¨æ–¼åˆ¤æ–·æ–‡å­—å’Œåœ–å½¢æ˜¯å¦ã€Œç·Šé„°ã€
NEIGHBOR_GAP_PX = 30
OUT_CHAPTERS_DIR = "structured_chapters_final"
INDEX_FILENAME = "chapters_index.json"
RAW_OUTLINE_FILENAME = "chapters_raw_outline.json"

# ==============================================================================
# === è¼”åŠ©å‡½å¼ï¼šæ•¸å­—èˆ‡é‚Šç•Œè™•ç† (ä¿ç•™) ===
# ==============================================================================

def chinese_to_arabic(chinese_num_str):
    """å°‡ä¸­æ–‡æ•¸å­—å­—ä¸²è½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—ã€‚"""
    conversion_map = {
        'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4,
        'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
        'å': 10
    }
    total = 0
    if not chinese_num_str:
        return None

    if 'å' in chinese_num_str:
        parts = chinese_num_str.split('å')
        if not parts[0]:
            total += 10
        else:
            total += conversion_map.get(parts[0], 0) * 10
        if len(parts) > 1 and parts[1]:
            total += conversion_map.get(parts[1], 0)
    else:
        total = conversion_map.get(chinese_num_str, None)

    return total

def get_iou(rect1, rect2):
    """è¨ˆç®— IoU (Intersection over Union)"""
    x1, y1, w1, h1 = rect1; x2, y2, w2, h2 = rect2
    xA = max(x1, x2); yA = max(y1, y2); xB = min(x1 + w1, x2 + w2); yB = min(y1 + h1, y2 + h2)
    inter_width = max(0, xB - xA); inter_height = max(0, yB - yA)
    intersection_area = inter_width * inter_height
    if intersection_area == 0: return 0.0
    area1 = w1 * h1; area2 = w2 * h2
    return intersection_area / (area1 + area2 - intersection_area)

def has_overlap(rect1, rect2):
    """
    æª¢æŸ¥å…©å€‹çŸ©å½¢æ˜¯å¦æœ‰é‡ç–Šï¼ˆæ•ˆèƒ½å„ªåŒ–ç‰ˆæœ¬ï¼‰
    æ¯” get_iou() > 0.0 æ›´å¿«ï¼Œå› ç‚ºä¸è¨ˆç®—é¢ç©å’Œæ¯”ä¾‹ï¼Œåªæª¢æŸ¥æ˜¯å¦æœ‰äº¤é›†
    """
    x1, y1, w1, h1 = rect1
    x2, y2, w2, h2 = rect2
    xA = max(x1, x2)
    yA = max(y1, y2)
    xB = min(x1 + w1, x2 + w2)
    yB = min(y1 + h1, y2 + h2)
    inter_width = max(0, xB - xA)
    inter_height = max(0, yB - yA)
    return inter_width > 0 and inter_height > 0

def filter_contained_figures(bboxes, overlap_threshold=0.9):
    """åœ–å½¢åŒ…å«éæ¿¾"""
    if not bboxes: return []
    kept_indices = list(range(len(bboxes)))
    for i in range(len(bboxes)):
        if i not in kept_indices: continue
        rect1 = bboxes[i]; x1, y1, w1, h1 = rect1; area1 = w1 * h1
        for j in range(len(bboxes)):
            if i == j or j not in kept_indices: continue
            rect2 = bboxes[j]; x2, y2, w2, h2 = rect2; area2 = w2 * h2
            if area1 == 0 or area2 == 0: continue
            xA = max(x1, x2); yA = max(y1, y2); xB = min(x1 + w1, x2 + w2); yB = min(y1 + h1, y2 + h2)
            intersection_area = max(0, xB - xA) * max(0, yB - yA)
            if intersection_area == 0: continue
            overlap_ratio_i_in_j = intersection_area / area1
            overlap_ratio_j_in_i = intersection_area / area2
            if overlap_ratio_i_in_j >= overlap_threshold and area1 < area2:
                if i in kept_indices: kept_indices.remove(i); break
            elif overlap_ratio_j_in_i >= overlap_threshold and area2 < area1:
                if j in kept_indices: kept_indices.remove(j)
    return [bboxes[i] for i in sorted(list(set(kept_indices)))]

def merge_overlapping_bboxes(bboxes):
    """é‚Šç•Œæ¡†æ“´å±•åˆä½µ"""
    if not bboxes: return []
    boxes = [[x, y, x + w, y + h] for x, y, w, h in bboxes]
    merged_boxes = []
    while boxes:
        current_box = boxes.pop(0)
        should_restart = True
        while should_restart:
            should_restart = False
            indices_to_merge = []
            for i in range(len(boxes)):
                other_box = boxes[i]
                xA = max(current_box[0], other_box[0]); yA = max(current_box[1], other_box[1])
                xB = min(current_box[2], other_box[2]); yB = min(current_box[3], other_box[3])
                if max(0, xB - xA) * max(0, yB - yA) > 0:
                    current_box[0] = min(current_box[0], other_box[0]); current_box[1] = min(current_box[1], other_box[1])
                    current_box[2] = max(current_box[2], other_box[2]); current_box[3] = max(current_box[3], other_box[3])
                    indices_to_merge.append(i); should_restart = True
            for i in sorted(indices_to_merge, reverse=True): del boxes[i]
        merged_boxes.append(current_box)
    return [(x0, y0, x1 - x0, y1 - y0) for x0, y0, x1, y1 in merged_boxes]


# ==============================================================================
# === è¼”åŠ©å‡½å¼ï¼šå‰µå»ºå®‰å…¨æª”å ===
# ==============================================================================
def create_safe_filename(text, max_len=50, extension=""):
    """
    å°‡æ¨™é¡Œæ–‡å­—è½‰æ›ç‚ºå®‰å…¨ã€ç°¡çŸ­ä¸”ç¬¦åˆæª”æ¡ˆç³»çµ±è¦å‰‡çš„éƒ¨åˆ†åç¨±ã€‚
    ä¿ç•™å®Œæ•´æ¨™é¡Œï¼ˆåŒ…å« Figure/Table ç·¨è™Ÿï¼‰ã€‚
    """
    if not text:
        return "untitled"

    # 1. ä¿ç•™å®Œæ•´æ¨™é¡Œï¼Œä¸ç§»é™¤ Figure/Table å‰ç¶´
    safe_text = text.strip()

    # 2. ç§»é™¤çµå°¾çš„çœç•¥è™Ÿã€å†’è™Ÿã€å¥è™Ÿç­‰æ¨™é»
    safe_text = TRAILING_PUNCTUATION_PATTERN.sub('', safe_text).strip()

    # 3. ç§»é™¤æ‰€æœ‰ä¸å®‰å…¨çš„å­—ç¬¦ (ä¿ç•™ï¼Œç”¨æ–¼æª”æ¡ˆåæ¸…ç†)
    safe_text = UNSAFE_FILENAME_CHARS_PATTERN.sub('', safe_text)

    # 4. å°‡ç©ºæ ¼æ›¿æ›ç‚ºåº•ç·š
    safe_text = WHITESPACE_PATTERN.sub('_', safe_text)

    # 5. å¦‚æœéé•·å‰‡æˆªæ–·
    safe_text = safe_text[:max_len]

    if not safe_text:
        return "untitled"

    # 6. ç¢ºä¿çµå°¾æ²’æœ‰å¤šé¤˜çš„åº•ç·š
    safe_text = safe_text.rstrip('_')

    return safe_text


# ==============================================================================
# *** dHash åœ–ç‰‡é›œæ¹Šå‡½å¼ ***
# ==============================================================================

def dhash(image, hash_size=8):
    """
    è¨ˆç®—åœ–ç‰‡çš„ Difference Hash (dHash)ã€‚
    """
    # è½‰æ›ç‚ºç°åº¦åœ– (å¦‚æœé‚„ä¸æ˜¯)
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image

    # ç¸®æ”¾åœ–ç‰‡åˆ° (hash_size + 1, hash_size)
    resized = cv2.resize(gray, (hash_size + 1, hash_size))

    # CPU ç‰ˆæœ¬
    diff = resized[:, 1:] > resized[:, :-1]
    return ''.join(str(int(b)) for b in diff.flatten())


def dhash_batch(images, hash_size=8):
    """
    æ‰¹æ¬¡è¨ˆç®—å¤šå¼µåœ–ç‰‡çš„ dHash

    åƒæ•¸ï¼š
        images: list of np.ndarrayï¼Œæ¯å¼µåœ–ç‰‡å¯ä»¥æ˜¯ BGR æˆ–ç°éš
        hash_size: hash å¤§å°ï¼ˆé è¨­ 8ï¼‰

    å‚³å›ï¼š
        list of strï¼Œæ¯å¼µåœ–ç‰‡çš„ hash å€¼ï¼ˆé †åºèˆ‡è¼¸å…¥ä¸€è‡´ï¼‰
    """
    if not images:
        return []

    # å¦‚æœåªæœ‰ 1 å¼µåœ–ï¼Œç›´æ¥å‘¼å«å–®å¼µç‰ˆæœ¬
    if len(images) == 1:
        return [dhash(images[0], hash_size)]

    # é è™•ç†ï¼šè½‰ç°éš + ç¸®æ”¾
    resized_batch = []
    for img in images:
        # è½‰ç°éš
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        # ç¸®æ”¾
        resized = cv2.resize(gray, (hash_size + 1, hash_size))
        resized_batch.append(resized)

    # å †ç–Šæˆ (N, H, W) çš„æ‰¹æ¬¡é™£åˆ—
    resized_batch = np.array(resized_batch, dtype=np.uint8)

    # CPU æ‰¹æ¬¡è™•ç†
    hashes = []
    for i in range(len(images)):
        diff = resized_batch[i, :, 1:] > resized_batch[i, :, :-1]
        hash_value = ''.join(str(int(b)) for b in diff.flatten())
        hashes.append(hash_value)

    return hashes


def rgb_to_gray_batch(images):
    """
    æ‰¹æ¬¡å°‡ RGB/BGR åœ–ç‰‡è½‰æ›ç‚ºç°éš

    åƒæ•¸ï¼š
        images: list of np.ndarrayï¼ŒBGR åœ–ç‰‡

    å‚³å›ï¼š
        list of np.ndarrayï¼Œç°éšåœ–ç‰‡ï¼ˆé †åºèˆ‡è¼¸å…¥ä¸€è‡´ï¼‰
    """
    if not images:
        return []

    # CPU ç‰ˆæœ¬
    return [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]


# ==============================================================================
# === çµæ§‹åŒ–æ“·å–èˆ‡ç›®éŒ„è§£æå‡½å¼ (å‚³å…¥ doc) ===
# ==============================================================================

def extract_text_blocks_from_page(page):
    """å¾æŒ‡å®šçš„ PDF é é¢æ“·å–æ–‡å­—å€å¡Š (0-based page_num)ã€‚"""
    try:
        return page.get_text('blocks')
    except Exception as e:
        return []

def find_catalog_pages(doc):
    """å°‹æ‰¾ PDF æª”æ¡ˆä¸­å¯èƒ½çš„ç›®éŒ„é é¢ã€‚"""
    catalog_pages = []
    found_catalog_start = False
    max_pages_to_check = min(len(doc), 15)

    # è¨­ç½®æ¨™æº–åŒ–çš„è‹±æ–‡é—œéµå­—ï¼ˆéƒ½ç”¨å¤§å¯«ï¼‰
    EN_CATALOG_KEYWORDS = ["CONTENTS", "TABLE OF CONTENTS", "INDEX"]

    for i in range(max_pages_to_check):
        page = doc[i]
        # å‡è¨­ extract_text_blocks_from_page å·²ç¶“å®šç¾©
        text_blocks = extract_text_blocks_from_page(page)

        # ç²å–é é¢å…¨éƒ¨æ–‡å­—
        text = "".join([block[4] for block in text_blocks])

        # æ ¸å¿ƒå„ªåŒ–ï¼šå°‡æ–‡æœ¬è½‰ç‚ºå¤§å¯«ä¸¦å°‡å¤šå€‹ç©ºç™½å­—å…ƒæ¨™æº–åŒ–ç‚ºå–®ä¸€ç©ºæ ¼
        # é€™èƒ½è§£æ±ºå¤§å°å¯«å•é¡Œå’Œ"Table of \n contents"çš„æ›è¡Œå•é¡Œ
        text_normalized = " ".join(text.upper().split())

        is_catalog = False

        # 1. é—œéµå­—åŒ¹é… (ä¸­/è‹±æ–‡)
        if "ç›®éŒ„" in text_normalized:
            is_catalog = True
        # æª¢æŸ¥è‹±æ–‡é—œéµå­— (ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„å¤§å¯«æ–‡æœ¬)
        elif any(keyword in text_normalized for keyword in EN_CATALOG_KEYWORDS):
            is_catalog = True

        # 2. æ­£å‰‡è¡¨é”å¼åŒ¹é… (å¦‚ï¼šé»é»é» + é ç¢¼)
        # å³ä½¿æ²’æœ‰æ˜ç¢ºçš„æ¨™é¡Œï¼Œä¹Ÿå¯ä»¥é€éæ’ç‰ˆç‰¹å¾µä¾†åˆ¤æ–·
        elif CATALOG_CHAPTER_PATTERN.search(text):
            is_catalog = True

        if is_catalog:
            catalog_pages.append((i, text))
            found_catalog_start = True
        else:
            # å¦‚æœå·²ç¶“é–‹å§‹æ‰¾åˆ°ç›®éŒ„é ï¼Œä½†ç•¶å‰é é¢ä¸å†æ˜¯ç›®éŒ„ï¼Œä¸”å…§å®¹é•·åº¦è¶…éä¸€å®šé™åˆ¶ï¼Œå‰‡åœæ­¢æœå°‹
            if found_catalog_start and text and len(text) > 100:
                break

    return catalog_pages

def parse_catalog_text(text):
    """
    å¾ç›®éŒ„æ–‡å­—ä¸­è§£æç« ç¯€æ¨™é¡Œå’Œé ç¢¼ã€‚
    """
    chapters = []
    if not text:
        return chapters

    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue

        m = CATALOG_LINE_PATTERN_DOTS.search(line)
        if not m:
            m = CATALOG_LINE_PATTERN_SPACE.search(line)

        if m:
            title_raw = m.group(1).strip()
            page_raw = m.group(2).strip()

            title = TRAILING_PUNCTUATION_PATTERN.sub('', title_raw).strip()

            page_int = None
            try:
                page_int = int(page_raw)
            except ValueError:
                page_int = chinese_to_arabic(page_raw)
            except Exception as e:
                pass
            if title and page_int is not None and len(title) > 2 and not title.isnumeric():
                chapters.append({"title": title, "page": page_int})
    return chapters

def find_y_coord_for_title(page, title_to_find, search_blocks):
    """åœ¨å–®é ä¸­å°‹æ‰¾ç‰¹å®šæ¨™é¡Œçš„ Y åº§æ¨™ (y0)ã€‚æ‰¾åˆ°å¤šå€‹åŒ¹é…æ™‚ï¼Œé¸æ“‡æœ€ä¸Šæ–¹çš„ï¼ˆY åº§æ¨™æœ€å°ï¼‰ã€‚"""
    search_title = title_to_find.strip()

    if len(search_title) > 30:
        search_title = search_title[:30]

    # æ­£è¦åŒ–ï¼šç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œï¼Œæ–¹ä¾¿åŒ¹é…
    search_normalized = " ".join(search_title.split())

    # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„ Y åº§æ¨™
    matched_y_coords = []

    # ç¬¬ä¸€è¼ªï¼šç²¾ç¢ºåŒ¹é…ï¼ˆå«é•·åº¦é™åˆ¶ï¼‰
    for b in search_blocks:
        x0, y0, x1, y1, text = b[:5]
        text_normalized = " ".join(text.split())

        if search_normalized in text_normalized and len(text.strip()) < len(search_title) + 50:
            matched_y_coords.append(y0)

    # å¦‚æœç¬¬ä¸€è¼ªæ‰¾åˆ°åŒ¹é…ï¼Œè¿”å›æœ€å°çš„ Y åº§æ¨™ï¼ˆæœ€ä¸Šæ–¹ï¼‰
    if matched_y_coords:
        return min(matched_y_coords)

    # ç¬¬äºŒè¼ªï¼šä¸å€åˆ†å¤§å°å¯«çš„åŒ¹é…
    search_title_lower = search_normalized.lower()
    for b in search_blocks:
        x0, y0, x1, y1, text = b[:5]
        text_normalized = " ".join(text.split()).lower()

        if search_title_lower in text_normalized:
            matched_y_coords.append(y0)

    # è¿”å›æœ€å°çš„ Y åº§æ¨™ï¼ˆæœ€ä¸Šæ–¹ï¼‰ï¼Œå¦‚æœæ²’æ‰¾åˆ°å‰‡è¿”å› None
    return min(matched_y_coords) if matched_y_coords else None

def find_outline(doc):
    """å˜—è©¦å¾ PDF çš„ Outline (æ›¸ç±¤) æŠ“å–ç« ç¯€çµæ§‹ã€‚"""
    chapters = []
    try:
        toc = doc.get_toc(simple=True)  # [level, title, page]
        for level, title, page in toc:
            if title and page > 0:
                cleaned_title = TRAILING_PUNCTUATION_PATTERN.sub('', title).strip()
                chapters.append({"title": cleaned_title, "page": page})
    except Exception as e:
        print(f"âš ï¸ åµæ¸¬ Outline ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    return chapters

# ==============================================================================
# === æ ¸å¿ƒè¼”åŠ©å‡½å¼ï¼šä¿®æ­£å¾Œçš„æ¨™é¡Œåµæ¸¬ (New/Modified) ===
# ==============================================================================

def find_figure_titles_from_reserved_blocks(
    potential_title_blocks, figure_blocks_px, scale_x, scale_y, page_height
):
    """
    å¾é å…ˆæ¨™è¨˜çš„ã€Œæ½›åœ¨æ¨™é¡Œã€å€å¡Šä¸­ï¼Œå°‹æ‰¾æœ€é è¿‘æ¯å€‹åœ–å½¢é‚Šç•Œæ¡†çš„æ–‡å­—ï¼Œ
    ä¸¦å°‡å…¶æ¨™è¨˜ç‚ºåœ–å½¢æ¨™é¡Œã€‚
    """
    figure_titles_map = {}

    for i, fig_rect_px in enumerate(figure_blocks_px):
        fx, fy, fw, fh = fig_rect_px

        # å°‡åœ–å½¢é‚Šç•Œæ¡†è½‰æ›ç‚º fitz åº§æ¨™
        fx0_fitz = fx / scale_x; fy0_fitz = fy / scale_y
        fx1_fitz = (fx + fw) / scale_x; fy1_fitz = (fy + fh) / scale_y

        best_title = None
        min_gap = float('inf')
        best_bbox = None

        for b in potential_title_blocks:
            tx0, ty0, tx1, ty1, text = b[:5]
            text = text.strip()

            # æª¢æŸ¥æ–‡å­—å€å¡Šæ˜¯å¦åœ¨åœ–å½¢ä¸Šæ–¹æˆ–ä¸‹æ–¹ç·Šé„°ï¼Œæˆ–é‡ç–Š
            gap_above = fy0_fitz - ty1
            gap_below = ty0 - fy1_fitz

            is_close = False

            # åˆ¤æ–·æ˜¯å¦é‡ç–Š (é‡ç–Šå³ç‚ºç·Šå¯†é—œè¯)
            if max(0, min(fx1_fitz, tx1) - max(fx0_fitz, tx0)) * max(0, min(fy1_fitz, ty1) - max(fy0_fitz, ty0)) > 0:
                 is_close = True

            # åˆ¤æ–·æ˜¯å¦ç·Šé„°ä¸”å°é½Š
            elif (0 <= gap_above <= NEIGHBOR_GAP_PX or 0 <= gap_below <= NEIGHBOR_GAP_PX):
                horizontal_overlap = max(0, min(fx1_fitz, tx1) - max(fx0_fitz, tx0))
                if horizontal_overlap > 0 or abs((fx0_fitz + fx1_fitz) / 2 - (tx0 + tx1) / 2) < NEIGHBOR_GAP_PX:
                    is_close = True

            if is_close:
                # åˆ¤æ–·èª°æœ€è¿‘ (çµ•å°å€¼è·é›¢ï¼Œé‡ç–Šæ™‚è·é›¢ç‚º 0)
                current_gap = 0
                if gap_above >= 0: current_gap = min(current_gap, gap_above)
                if gap_below >= 0: current_gap = min(current_gap, gap_below)

                if current_gap < min_gap:
                    min_gap = current_gap
                    best_title = text
                    best_bbox = (tx0, ty0, tx1, ty1)

        if best_title:
            figure_titles_map[f"figure_{i+1}"] = {
                "content": best_title,
                "bbox": best_bbox
            }

    return figure_titles_map

def process_single_page_and_get_items(
    page, page_height, chapter_assets_dir, chapter_safe_name,
    start_y_coordinate=None, end_y_coordinate=None, dpi=200, debug_draw=True,
    camelot_tables=[], chapter_image_cache=None, cache_lock=None
):
    """
    è™•ç†å–®é  PDFï¼Œæ“·å–æ–‡å­—ã€è¡¨æ ¼ã€åœ–å½¢ï¼Œä¸¦å›å‚³æ’åºå¾Œçš„é …ç›®åˆ—è¡¨ã€‚
    é—œéµä¿®æ­£ï¼šåœ¨åœ–å½¢åˆä½µå‰ï¼Œå…ˆéš”é›¢æ¨™é¡Œæ–‡å­—ã€‚
    """
    global IMAGE_HASH_CACHE

    # ä½¿ç”¨å‚³å…¥çš„ç« ç¯€å°ˆå±¬å¿«å–ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨å…¨åŸŸå¿«å–
    if chapter_image_cache is None:
        image_cache = IMAGE_HASH_CACHE
        lock = threading.Lock()  # å…¨åŸŸå¿«å–ä½¿ç”¨è‡ªå·±çš„é–
    else:
        image_cache = chapter_image_cache
        lock = cache_lock if cache_lock else threading.Lock()

    page_num = page.number
    page_width = page.rect.width
    items_out = []
    successful_table_bboxes_fitz = []
    figure_blocks_px = []
    text_blocks_fitz = []
    chapter_title_bbox_fitz = None
    occupied_text_bboxes_fitz = [] # å„²å­˜æ‰€æœ‰éæ­£æ–‡æ–‡å­— (è¡¨æ ¼æ¨™é¡Œã€åœ–å½¢æ¨™é¡Œ) çš„é‚Šç•Œæ¡†

    pix = page.get_pixmap(dpi=dpi)
    img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)
    if pix.n == 4: img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
    elif pix.n == 3: img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

    img_height, img_width, _ = img.shape
    scale_x = img_width / page_width
    scale_y = img_height / page_height

    # --- [1/6] åµæ¸¬èˆ‡éš”é›¢æ¨™é¡Œæ–‡å­—å€å¡Š (FIT) ---
    text_blocks_raw = page.get_text("blocks")
    potential_figure_titles_blocks = []

    for b in text_blocks_raw:
        x0, y0, x1, y1, text = b[:5]
        text = text.strip()
        if not text:
            continue

        # æª¢æŸ¥æ˜¯å¦ç‚ºç« ç¯€æ¨™é¡Œ
        if start_y_coordinate is not None and abs(y0 - start_y_coordinate) < 5:
            chapter_title_bbox_fitz = (x0, y0, x1, y1)

        # æª¢æŸ¥æ˜¯å¦ç‚ºæ½›åœ¨çš„åœ–è¡¨æ¨™é¡Œï¼ˆå³ä½¿è¢«ç•«é€²åœ–å½¢ï¼Œä¹Ÿå…ˆä¿ç•™ï¼‰
        # æ”¯æ´ï¼šç¹é«”ä¸­æ–‡ã€ç°¡é«”ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ“æ–‡ç­‰
        if FIGURE_TABLE_TITLE_PATTERN.match(text):
            # å°‡å®ƒæ¨™è¨˜ç‚ºæ½›åœ¨æ¨™é¡Œ
            potential_figure_titles_blocks.append(b)
            occupied_text_bboxes_fitz.append((x0, y0, x1, y1))
            continue # æ¨™é¡Œæ–‡å­—ä¸é€²å…¥å¸¸è¦ text_blocks_fitz

        # éæ¿¾é çœ‰é è…³ï¼Œéæ¨™é¡Œæ–‡å­—æ‰æœƒè¢«è™•ç†
        if start_y_coordinate is not None and y1 < start_y_coordinate:
            continue
        if end_y_coordinate is not None and y0 >= end_y_coordinate:
            continue

        text_blocks_fitz.append(b)

    # --- [2/6] è™•ç†è¡¨æ ¼ ---
    detected_tables_info = []
    for i, t in enumerate(camelot_tables):
        x0, y0, x1, y1 = map(float, t._bbox)
        table_top_y = page_height - y1
        table_bottom_y = page_height - y0

        is_valid_table = True
        # ä¿®æ­£ï¼šå¦‚æœè¡¨æ ¼çš„åº•éƒ¨ï¼ˆæœ€ä¸‹æ–¹ï¼‰åœ¨ç« ç¯€é–‹å§‹åº§æ¨™ä¹‹å‰ï¼Œå‰‡æ’é™¤æ•´å€‹è¡¨æ ¼
        # è¡¨æ ¼çš„ä»»ä½•éƒ¨åˆ†éƒ½ä¸æ‡‰è©²åœ¨ç« ç¯€é–‹å§‹ä¹‹å‰
        if start_y_coordinate is not None and table_bottom_y < start_y_coordinate:
            is_valid_table = False
        # å¦‚æœè¡¨æ ¼çš„é ‚éƒ¨ï¼ˆæœ€ä¸Šæ–¹ï¼‰åœ¨ç« ç¯€çµæŸåº§æ¨™ä¹‹å¾Œï¼Œå‰‡æ’é™¤
        if end_y_coordinate is not None and table_top_y > end_y_coordinate:
            is_valid_table = False
        if not is_valid_table:
            continue

        area = (x1 - x0) * (y1 - y0)
        df = t.df
        is_valid_csv = not (df.shape[0] < 2 or df.shape[1] < 2 or (df.replace("", pd.NA).isna().sum().sum() / max(1, df.size) > 0.5))

        if is_valid_csv:
            detected_tables_info.append({
                "camelot_table": t, "bbox": (x0, y0, x1, y1), "area": area, "name": f"table_{i+1}"
            })

    filtered_tables_info = []
    for t_info in detected_tables_info:
        t = t_info["camelot_table"]
        x0_c, y0_c, x1_c, y1_c = t_info["bbox"]
        name = t_info["name"]
        y_raw_a = page_height - y0_c
        y_raw_b = page_height - y1_c

        x0_fitz = max(0, x0_c)
        x1_fitz = min(page_width, x1_c)
        y0_fitz = max(0, min(page_height, y_raw_b))
        y1_fitz = max(0, min(page_height, y_raw_a))
        table_height = y1_fitz - y0_fitz
        if table_height <= 1.0: continue

        if end_y_coordinate is not None and y0_fitz >= end_y_coordinate:
             continue

        filtered_tables_info.append(t_info)
        successful_table_bboxes_fitz.append((x0_fitz, y0_fitz, x1_fitz, y1_fitz, name))
        occupied_text_bboxes_fitz.append((x0_fitz, y0_fitz, x1_fitz, y1_fitz)) # å°‡è¡¨æ ¼é‚Šç•ŒåŠ å…¥ä½”ç”¨åˆ—è¡¨

    for t_info in filtered_tables_info:
        t = t_info["camelot_table"]
        x0_c, y0_c, x1_c, y1_c = t_info["bbox"]
        y_center = (page_height - y1_c + page_height - y0_c) / 2
        items_out.append({
            "type": "table", "y_center": y_center, "content": t.df, "name": t_info['name'], "mode": "csv", "page_num": page_num + 1
        })


    # --- [3/6] åµæ¸¬èˆ‡åˆä½µåœ–å½¢ ---
    img_for_opencv = img.copy()
    if successful_table_bboxes_fitz:
        for tx0, ty0, tx1, ty1, _ in successful_table_bboxes_fitz:
            pt1_px = (int(tx0 * scale_x), int(ty0 * scale_y))
            pt2_px = (int(tx1 * scale_x), int(ty1 * scale_y))
            cv2.rectangle(img_for_opencv, pt1_px, pt2_px, (255, 255, 255), -1)

    # åœ–åƒè™•ç† (CPU)
    gray = cv2.cvtColor(img_for_opencv, cv2.COLOR_BGR2GRAY)

    _, th = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)
    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    temp_opencv_bboxes = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if w <= 100 or h <= 80: continue

        # éæ¿¾å–®è¡Œæ–‡å­—æ¡†ï¼šå¯¬é«˜æ¯”éå¤§ï¼ˆå¤ªæ‰ï¼‰çš„æ¡†ä¸ç•¶ä½œåœ–ç‰‡                  â•â”‚
        aspect_ratio = w / h if h > 0 else 0
        if aspect_ratio > 10:  # å¯¬åº¦è¶…éé«˜åº¦10å€ï¼Œè¦–ç‚ºå–®è¡Œæ–‡å­—             â•â”‚
            continue

        temp_opencv_bboxes.append((x, y, w, h))

    valid_opencv_bboxes = filter_contained_figures(temp_opencv_bboxes, overlap_threshold=0.9)
    merged_bboxes = merge_overlapping_bboxes(valid_opencv_bboxes)

    # æ•ˆèƒ½å„ªåŒ–ï¼šé å…ˆè¨ˆç®—æ‰€æœ‰æ–‡å­—é‚Šç•Œæ¡†çš„åƒç´ åº§æ¨™ï¼Œé¿å…åœ¨æ¯å€‹åœ–å½¢è¿´åœˆä¸­é‡è¤‡è¨ˆç®—
    # ç¯€çœï¼šè‹¥æœ‰ N å€‹åœ–å½¢å’Œ M å€‹æ–‡å­—ï¼Œå¾ O(NÃ—M) æ¬¡è¨ˆç®—é™ç‚º O(M) æ¬¡
    text_bboxes_px = [(int(b[0]*scale_x), int(b[1]*scale_y), int((b[2]-b[0])*scale_x), int((b[3]-b[1])*scale_y)) for b in text_blocks_fitz]

    # æ•ˆèƒ½å„ªåŒ–ï¼šé å…ˆè¨ˆç®—æ‰€æœ‰è¡¨æ ¼é‚Šç•Œæ¡†çš„åƒç´ åº§æ¨™ï¼Œé¿å…åœ¨åœ–å½¢æ“´å¼µè¿´åœˆä¸­é‡è¤‡è¨ˆç®—
    # ç¯€çœï¼šè‹¥æœ‰ N å€‹åœ–å½¢ã€M å€‹æ–‡å­—ã€T å€‹è¡¨æ ¼ï¼Œå¾ O(NÃ—MÃ—T) æ¬¡è¨ˆç®—é™ç‚º O(T) æ¬¡
    # æ³¨æ„ï¼šsuccessful_table_bboxes_fitz æ˜¯ (x0, y0, x1, y1, name) äº”å…ƒçµ„
    table_bboxes_px = [
        (int(tb[0] * scale_x), int(tb[1] * scale_y), int(tb[2] * scale_x), int(tb[3] * scale_y))
        for tb in successful_table_bboxes_fitz  # tb[0:4] å–å‰å››å€‹åº§æ¨™å€¼
    ]

    combined_bboxes_with_text = []
    for fx, fy, fw, fh in merged_bboxes:
        fx_fitz = fx / scale_x
        fy_fitz = fy / scale_y
        fw_fitz = fw / scale_x
        fh_fitz = fh / scale_y

        # æª¢æŸ¥åœ–å½¢èµ·å§‹ä½ç½®æ˜¯å¦åœ¨ç« ç¯€ç¯„åœä¹‹å‰
        if start_y_coordinate is not None and (fy_fitz + fh_fitz) <= start_y_coordinate:
            continue

        # æª¢æŸ¥åœ–å½¢èµ·å§‹ä½ç½®æ˜¯å¦å®Œå…¨åœ¨ç« ç¯€ç¯„åœä¹‹å¾Œ
        if end_y_coordinate is not None and fy_fitz >= end_y_coordinate:
            continue

        x_min, y_min, x_max, y_max = fx, fy, fx + fw, fy + fh
        rect_fig = (fx, fy, fw, fh)

        # ç¬¬ä¸€æ¬¡æ“´å¼µï¼ˆä½¿ç”¨é å…ˆè¨ˆç®—å¥½çš„ text_bboxes_pxï¼‰
        for tx, ty, tw, th in text_bboxes_px:
            rect_text = (tx, ty, tw, th)
            tx_min, ty_min, tx_max, ty_max = tx, ty, tx + tw, ty + th

            # æª¢æŸ¥æ–‡å­—å€å¡Šæ˜¯å¦åœ¨è¡¨æ ¼å…§ï¼Œå¦‚æœæ˜¯å‰‡è·³éï¼ˆä¸èˆ‡åœ–å½¢åˆä½µï¼‰
            # æ•ˆèƒ½å„ªåŒ–ï¼šåœ¨è¡¨æ ¼è¿´åœˆå¤–é å…ˆè¨ˆç®—æ–‡å­—ä¸­å¿ƒé»ï¼Œé¿å…æ¯å€‹è¡¨æ ¼éƒ½é‡è¤‡è¨ˆç®—
            text_center_x = (tx_min + tx_max) / 2
            text_center_y = (ty_min + ty_max) / 2
            is_text_in_table = False
            for table_x0_px, table_y0_px, table_x1_px, table_y1_px in table_bboxes_px:
                # æª¢æŸ¥æ–‡å­—ä¸­å¿ƒé»æ˜¯å¦åœ¨è¡¨æ ¼å…§
                if table_x0_px <= text_center_x <= table_x1_px and table_y0_px <= text_center_y <= table_y1_px:
                    is_text_in_table = True
                    break

            if is_text_in_table:
                continue  # è·³éè¡¨æ ¼å…§çš„æ–‡å­—ï¼Œä¸èˆ‡åœ–å½¢åˆä½µ

            # æ•ˆèƒ½å„ªåŒ–ï¼šä½¿ç”¨ has_overlap() å–ä»£ get_iou() > 0.0ï¼Œçœç•¥é¢ç©å’Œæ¯”ä¾‹è¨ˆç®—
            is_inside_or_overlapping = has_overlap(rect_fig, rect_text)
            is_outside_and_close = False

            if not is_inside_or_overlapping:
                vertical_gap = min(abs(ty_max - y_min), abs(ty_min - y_max))
                horizontal_overlap_width = max(0, min(tx_max, x_max) - max(tx_min, x_min))
                is_vertically_aligned = (vertical_gap <= NEIGHBOR_GAP_PX) and (horizontal_overlap_width > 0)
                horizontal_gap = min(abs(tx_max - x_min), abs(tx_min - x_max))
                vertical_overlap_height = max(0, min(ty_max, y_max) - max(ty_min, y_min))
                is_horizontally_aligned = (horizontal_gap <= NEIGHBOR_GAP_PX) and (vertical_overlap_height > 0)
                if is_vertically_aligned or is_horizontally_aligned: is_outside_and_close = True

            if is_inside_or_overlapping or is_outside_and_close:
                x_min = min(x_min, tx_min); y_min = min(y_min, ty_min)
                x_max = max(x_max, tx_max); y_max = max(y_max, ty_max)

        # æª¢æŸ¥ç¬¬ä¸€æ¬¡æ“´å¼µå¾Œçš„é‚Šç•Œæ˜¯å¦ã€Œå£“åˆ°ã€ä»»ä½•æ–‡å­—
        has_overlapping_text = False
        for tx, ty, tw, th in text_bboxes_px:
            tx_min, ty_min, tx_max, ty_max = tx, ty, tx + tw, ty + th
            # æª¢æŸ¥æ˜¯å¦æœ‰é‡ç–Šï¼ˆå³ä½¿éƒ¨åˆ†é‡ç–Šä¹Ÿç®—ï¼‰
            if max(0, min(x_max, tx_max) - max(x_min, tx_min)) > 0 and max(0, min(y_max, ty_max) - max(y_min, ty_min)) > 0:
                has_overlapping_text = True
                break

        # ç¬¬äºŒæ¬¡æ“´å¼µï¼šåªæœ‰åœ¨ç¬¬ä¸€æ¬¡æ“´å¼µå¾Œã€Œå£“åˆ°ã€æ–‡å­—æ™‚æ‰åŸ·è¡Œ
        if has_overlapping_text:
            for tx, ty, tw, th in text_bboxes_px:
                tx_min, ty_min, tx_max, ty_max = tx, ty, tx + tw, ty + th

                # æª¢æŸ¥æ–‡å­—æ˜¯å¦åœ¨è¡¨æ ¼å…§
                # æ•ˆèƒ½å„ªåŒ–ï¼šåœ¨è¡¨æ ¼è¿´åœˆå¤–é å…ˆè¨ˆç®—æ–‡å­—ä¸­å¿ƒé»ï¼Œé¿å…æ¯å€‹è¡¨æ ¼éƒ½é‡è¤‡è¨ˆç®—
                text_center_x = (tx_min + tx_max) / 2
                text_center_y = (ty_min + ty_max) / 2
                is_text_in_table = False
                for table_x0_px, table_y0_px, table_x1_px, table_y1_px in table_bboxes_px:
                    if table_x0_px <= text_center_x <= table_x1_px and table_y0_px <= text_center_y <= table_y1_px:
                        is_text_in_table = True
                        break

                if is_text_in_table:
                    continue

                # ä½¿ç”¨æ“´å¼µå¾Œçš„é‚Šç•Œé€²è¡Œæª¢æŸ¥
                rect_current = (x_min, y_min, x_max - x_min, y_max - y_min)
                rect_text = (tx, ty, tw, th)

                # æ•ˆèƒ½å„ªåŒ–ï¼šä½¿ç”¨ has_overlap() å–ä»£ get_iou() > 0.0ï¼Œçœç•¥é¢ç©å’Œæ¯”ä¾‹è¨ˆç®—
                is_inside_or_overlapping = has_overlap(rect_current, rect_text)
                is_outside_and_close = False

                if not is_inside_or_overlapping:
                    vertical_gap = min(abs(ty_max - y_min), abs(ty_min - y_max))
                    horizontal_overlap_width = max(0, min(tx_max, x_max) - max(tx_min, x_min))
                    is_vertically_aligned = (vertical_gap <= NEIGHBOR_GAP_PX) and (horizontal_overlap_width > 0)
                    horizontal_gap = min(abs(tx_max - x_min), abs(tx_min - x_max))
                    vertical_overlap_height = max(0, min(ty_max, y_max) - max(ty_min, y_min))
                    is_horizontally_aligned = (horizontal_gap <= NEIGHBOR_GAP_PX) and (vertical_overlap_height > 0)
                    if is_vertically_aligned or is_horizontally_aligned: is_outside_and_close = True

                if is_inside_or_overlapping or is_outside_and_close:
                    x_min = min(x_min, tx_min); y_min = min(y_min, ty_min)
                    x_max = max(x_max, tx_max); y_max = max(y_max, ty_max)

        final_x = max(0, x_min); final_y = max(0, y_min)
        final_w = min(img_width, x_max) - final_x; final_h = min(img_height, y_max) - final_y

        # ä¸é€²è¡Œä»»ä½•è£åˆ‡ï¼Œä¿ç•™å®Œæ•´åœ–å½¢
        # è·¨ç« ç¯€çš„åœ–ç‰‡æœƒåœ¨å…©å€‹ç« ç¯€éƒ½å®Œæ•´å‡ºç¾
        combined_bboxes_with_text.append((final_x, final_y, final_w, final_h))

    final_merged_cutouts = merge_overlapping_bboxes(combined_bboxes_with_text)
    figure_blocks_px = final_merged_cutouts


    # --- [4/6] åµæ¸¬åœ–å½¢æ¨™é¡Œ (å¾é å…ˆä¿ç•™çš„å€å¡Šä¸­å°‹æ‰¾) ---
    figure_titles_map = find_figure_titles_from_reserved_blocks(
        potential_figure_titles_blocks, # ä½¿ç”¨é å…ˆç¯©é¸çš„æ¨™é¡Œå€å¡Š
        figure_blocks_px,
        scale_x, scale_y, page_height
    )

    # --- [5/6] å„²å­˜åœ–å½¢ã€å‘½åä¸¦å»é‡è¤‡ (ä½¿ç”¨æ‰¹æ¬¡ dHash) ---
    figure_items_to_add = []

    # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰æœ‰æ•ˆçš„ ROI åœ–ç‰‡å’Œç´¢å¼•
    valid_rois = []
    valid_indices = []
    valid_bboxes = []

    for i, (x, y, w, h) in enumerate(final_merged_cutouts):
        if w <= 100 or h <= 80:
            continue

        roi = img[y:y+h, x:x+w]
        valid_rois.append(roi)
        valid_indices.append(i)
        valid_bboxes.append((x, y, w, h))

    # ç¬¬äºŒæ­¥ï¼šæ‰¹æ¬¡è¨ˆç®—æ‰€æœ‰åœ–ç‰‡çš„ hash
    if valid_rois:
        current_hashes = dhash_batch(valid_rois)
    else:
        current_hashes = []

    # ç¬¬ä¸‰æ­¥ï¼šè™•ç†æ¯å¼µåœ–ç‰‡ï¼ˆé †åºèˆ‡è¼¸å…¥ä¸€è‡´ï¼‰
    for idx, (i, roi, current_hash, (x, y, w, h)) in enumerate(zip(valid_indices, valid_rois, current_hashes, valid_bboxes)):

        # ä½¿ç”¨é–ä¿è­·å¿«å–çš„è®€å–
        with lock:
            is_duplicate = current_hash in image_cache
            if is_duplicate:
                first_saved_path = image_cache[current_hash]

        if is_duplicate:
            original_filename = os.path.basename(first_saved_path)
            y_center_fitz = (y / scale_y + (y + h) / scale_y) / 2

            figure_items_to_add.append({
                "type": "figure",
                "y_center": y_center_fitz,
                "content": first_saved_path,
                "name": original_filename,
                "mode": "jpg",
                "page_num": page_num + 1,
                "title": f"[é‡è¤‡åœ–å½¢ï¼Œä½¿ç”¨: {original_filename}]"
            })
            print(f"   [å»é‡] é é¢ {page_num + 1} çš„åœ–å½¢ {i + 1} (Hash: {current_hash[:10]}...) ç‚ºé‡è¤‡åœ–å½¢ï¼Œä½¿ç”¨é¦–æ¬¡å‡ºç¾çš„æª”æ¡ˆ: {original_filename}")
            continue

        title_content = figure_titles_map.get(f"figure_{i+1}", {}).get("content", "")
        safe_title_part = create_safe_filename(title_content, max_len=40)

        fig_filename_base = f"{chapter_safe_name}_page{page_num+1}_{safe_title_part}"
        fig_filename = fig_filename_base + ".jpg"

        counter = 1
        current_fig_path = os.path.join(chapter_assets_dir, fig_filename)
        base_name_no_ext = os.path.join(chapter_assets_dir, fig_filename_base)
        while os.path.exists(current_fig_path):
            current_fig_path = f"{base_name_no_ext}_{counter}.jpg"
            counter += 1

        cv2.imwrite(current_fig_path, roi)

        # ä½¿ç”¨é–ä¿è­·å¿«å–çš„å¯«å…¥
        with lock:
            image_cache[current_hash] = current_fig_path

        y_center_fitz = (y / scale_y + (y + h) / scale_y) / 2

        figure_items_to_add.append({
            "type": "figure",
            "y_center": y_center_fitz,
            "content": current_fig_path,
            "name": os.path.basename(current_fig_path),
            "mode": "jpg",
            "page_num": page_num + 1,
            "title": title_content
        })

    items_out.extend(figure_items_to_add)

    # --- [6/6] è™•ç†ç´”æ–‡å­— (å·²æ’é™¤æ‰€æœ‰æ¨™é¡Œå’Œè¡¨æ ¼ä½”ç”¨å€åŸŸçš„æ–‡å­—) ---

    for b in text_blocks_fitz: # é€™è£¡çš„ text_blocks_fitz å·²æ’é™¤æ½›åœ¨æ¨™é¡Œ
        x0, y0, x1, y1, text = b[:5]
        if not text.strip(): continue
        y_center = (y0 + y1) / 2; x_center = (x0 + x1) / 2
        text_bbox_fitz = (x0, y0, x1, y1)

        is_chapter_title = (chapter_title_bbox_fitz is not None and abs(y0 - chapter_title_bbox_fitz[1]) < 5)

        inside_element = False

        # æ’é™¤åœ¨è¡¨æ ¼æˆ–åœ–å½¢é‚Šç•Œå…§çš„æ–‡å­—
        for fx0, fy0, fx1, fy1 in occupied_text_bboxes_fitz:
            # æª¢æŸ¥æ–‡å­—ä¸­å¿ƒé»æ˜¯å¦åœ¨ä½”ç”¨å€å…§
            if fx0 <= x_center <= fx1 and fy0 <= y_center <= fy1:
                inside_element = True; break
        if inside_element: continue

        # æ’é™¤è¢«åœ–å½¢ä½”ç”¨çš„æ–‡å­— (ä½¿ç”¨æœ€çµ‚åˆä½µå¾Œçš„åœ–å½¢é‚Šç•Œ)
        for x_cv, y_cv, w_cv, h_cv in figure_blocks_px:
            fx0 = x_cv / scale_x; fy0 = y_cv / scale_y
            fx1 = (x_cv + w_cv) / scale_x; fy1 = (y_cv + h_cv) / scale_y
            if fx0 <= x_center <= fx1 and fy0 <= y_center <= fy1:
                inside_element = True; break
        if inside_element: continue

        items_out.append({
            "type": "text",
            "y_center": y_center,
            "content": text.strip(),
            "page_num": page_num + 1,
            "is_title": is_chapter_title
        })

    # [7/7] Debug ç¹ªåœ– (ä¿ç•™ï¼Œä½†ç¢ºä¿ä½¿ç”¨çš„æ˜¯æ­£ç¢ºçš„é‚Šç•Œæ¡†)
    if debug_draw:
        img_debug = img.copy()

        # è—è‰²ï¼šæ‰€æœ‰åŸå§‹æ–‡å­—å€å¡Š
        for b in text_blocks_raw:
            x0, y0, x1, y1 = b[:4]
            pt1 = (int(x0 * scale_x), int(y0 * scale_y))
            pt2 = (int(x1 * scale_x), int(y1 * scale_y))
            cv2.rectangle(img_debug, pt1, pt2, (255, 0, 0), 1)

        # ç´…è‰²ï¼šè¡¨æ ¼å€å¡Š
        for (tx0, ty0, tx1, ty1, name) in successful_table_bboxes_fitz:
            pt1 = (int(tx0 * scale_x), int(ty0 * scale_y))
            pt2 = (int(tx1 * scale_x), int(ty1 * scale_y))
            cv2.rectangle(img_debug, pt1, pt2, (0, 0, 255), 2)
            cv2.putText(img_debug, name, (pt1[0], max(0, pt1[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)

        # ç¶ è‰²ï¼šæœ€çµ‚åœ–å½¢è£å‰ªå€å¡Š
        for x, y, w, h in figure_blocks_px:
            cv2.rectangle(img_debug, (x, y), (x + w, y + h), (0, 255, 0), 2)

        # ç´«è‰²ï¼šå·²è­˜åˆ¥çš„åœ–å½¢æ¨™é¡Œå€å¡Š
        for name, title_info in figure_titles_map.items():
            x0, y0, x1, y1 = title_info['bbox']
            pt1 = (int(x0 * scale_x), int(y0 * scale_y))
            pt2 = (int(x1 * scale_x), int(y1 * scale_y))
            cv2.rectangle(img_debug, pt1, pt2, (255, 0, 255), 2)
            cv2.putText(img_debug, "Fig. Title", (pt1[0], max(0, pt1[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2, cv2.LINE_AA)

        # é»ƒè‰²ï¼šç« ç¯€æ¨™é¡Œ
        if chapter_title_bbox_fitz:
            x0, y0, x1, y1 = chapter_title_bbox_fitz
            pt1 = (int(x0 * scale_x), int(y0 * scale_y))
            pt2 = (int(x1 * scale_x), int(y1 * scale_y))
            cv2.rectangle(img_debug, pt1, pt2, (0, 255, 255), 2)
            cv2.putText(img_debug, "Chapter Title", (pt1[0], max(0, pt1[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)

        if end_y_coordinate:
            y_px = int(end_y_coordinate * scale_y)
            cv2.line(img_debug, (0, y_px), (img_width, y_px), (0, 255, 255), 2)
            cv2.putText(img_debug, "Chapter End", (10, max(0, y_px - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)

        debug_img_path = os.path.join(chapter_assets_dir, f"page{page_num+1}_debug.jpg")
        cv2.imwrite(debug_img_path, img_debug)
        print(f"âœ… Debug åœ–è¼¸å‡ºï¼š{debug_img_path}")

    return items_out


# ==============================================================================
# === å¾ŒçºŒç« ç¯€è™•ç†å‡½å¼ (å·²åŠ å…¥é ç¢¼ä½ç§»æ ¡æ­£çš„ Print) ===
# ==============================================================================

def build_chapters_index(doc, catalog_data, raw_chapters_list, out_dir="chapters_index_only", source="toc"):
    """
    æ ¹æ“šç›®éŒ„/å¤§ç¶±æ•¸æ“šå»ºç«‹ç« ç¯€çš„é ç¢¼ç´¢å¼•ï¼Œä¸¦é‡å°åŒé ç« ç¯€é€²è¡Œ Y åº§æ¨™æ’åºã€‚
    (å·²æ¢å¾©åŸå§‹çš„å…©æ®µå¼æ ¡æ­£é‚è¼¯ï¼Œä¸¦åŠ å…¥è©³ç´° Print è¼¸å‡º)

    åƒæ•¸:
        catalog_data: ç›®éŒ„æ•¸æ“š (ç›´æ¥å‚³éï¼Œä¸å†å¾æª”æ¡ˆè®€å–)
        raw_chapters_list: åŸå§‹ç« ç¯€åˆ—è¡¨ (ç›´æ¥å‚³éï¼Œä¸å†å¾æª”æ¡ˆè®€å–)
    """
    # å·²åœç”¨ chapters_index_only è³‡æ–™å¤¾è¼¸å‡º
    # os.makedirs(out_dir, exist_ok=True)
    # idx_file = os.path.join(out_dir, INDEX_FILENAME)

    # åœ¨å‡½å¼é–‹é ­è¨ˆç®—ä¸€æ¬¡ï¼Œé¿å…é‡è¤‡è¨ˆç®—
    total_pages = len(doc)

    results_index = []
    all_chapters = sorted(raw_chapters_list, key=lambda x: x['page'])


    # --- æ­¥é©Ÿ 2: é ç¢¼ä½ç§»è£œæ­£é‚è¼¯ (å·²çµåˆæ‚¨çš„è¦æ±‚) ---
    print("\n--- åŸ·è¡Œé ç¢¼ä½ç§»è£œæ­£é‚è¼¯ (TOC æ¨¡å¼) ---")

    # âš ï¸ åˆå§‹åŒ–ä½ç§»é‡ï¼Œé è¨­ç‚º 0
    offset = 0

    if source == "toc" and catalog_data and all_chapters:
        first_catalog_page = catalog_data[0]["catalog_page"] # ç›®éŒ„æ‰€åœ¨é  (1-based)
        first_chap_title = all_chapters[0]["title"] # <-- åµæ¸¬çš„éŒ¨é»æ¨™é¡Œ
        old_first = all_chapters[0]["page"] # ç›®éŒ„/TOC è§£æå‡ºçš„èµ·å§‹é ç¢¼

        # ã€2.1 éšæ®µï¼šå¼·åˆ¶é ä½ç§»ã€‘
        print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ1ã€‘åµæ¸¬åˆ° Source=TOCï¼Œç›®éŒ„é ç¢¼: {first_catalog_page}ï¼ŒTOCè§£æçš„é¦–ç« é ç¢¼: {old_first}")

        if old_first <= first_catalog_page:
            offset = (first_catalog_page + 1) - old_first # âš ï¸ è¨ˆç®—ä½ç§»é‡
            print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ1ã€‘âš ï¸ åµæ¸¬åˆ°é¦–ç« é ç¢¼ ({old_first}) ä½æ–¼æˆ–æ—©æ–¼ç›®éŒ„é  ({first_catalog_page})ã€‚")
            print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ1ã€‘åŸ·è¡Œ**å¼·åˆ¶é ä½ç§»**ï¼šå°‡æ‰€æœ‰é ç¢¼å‘å¾Œæ¨é² **+{offset}** é ã€‚")

            for chap in all_chapters:
                chap["page"] += offset

            new_offset_start = all_chapters[0]["page"]
            print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ1ã€‘æ–°çš„é¦–ç« èµ·å§‹é ç¢¼ç‚º: {new_offset_start}")
        else:
            new_offset_start = old_first
            print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ1ã€‘é¦–ç« é ç¢¼ ({old_first}) åœ¨ç›®éŒ„é ä¹‹å¾Œï¼Œè·³éå¼·åˆ¶é ä½ç§»ã€‚")

        # ã€2.2 éšæ®µï¼šå¾®èª¿æ ¡æ­£ - é—œéµå­—æ¨¡ç³Šåµæ¸¬ã€‘
        max_pages = len(doc)
        found_valid_title = False
        candidate_start = all_chapters[0]["page"] # é ä½ç§»å¾Œçš„æ–°é ç¢¼

        # 1. æå–æ ¸å¿ƒé—œéµå­—
        # ç§»é™¤æ•¸å­—ç·¨è™Ÿ (å¦‚: 1, 1.1) å’Œå¤šé¤˜ç©ºæ ¼
        core_keyword_raw = CHAPTER_NUMBER_PREFIX_PATTERN.sub("", first_chap_title).strip()
        # ç§»é™¤å¸¸è¦‹çš„ç« /ç¯€/åœ–/è¡¨å‰ç¶´
        core_keyword = CHAPTER_PREFIX_PATTERN.sub("", core_keyword_raw).strip()

        if len(core_keyword) < 3: # å¦‚æœé—œéµå­—å¤ªçŸ­ï¼ˆä¾‹å¦‚åªæœ‰å…©å€‹å­—ï¼‰ï¼Œä½¿ç”¨åŸå§‹æ¨™é¡Œ
            search_keyword = re.escape(first_chap_title.strip())
        else:
            # ä½¿ç”¨æ ¸å¿ƒé—œéµå­—ï¼Œä¸”å…è¨±å‰å¾Œæœ‰ä»»æ„æ–‡å­—
            search_keyword = re.escape(core_keyword)

        # 2. å®šç¾©åµæ¸¬ Regex (çµ„åˆï¼š[ç¬¬Xç« ] OR [æ ¸å¿ƒé—œéµå­—])
        # ä½¿ç”¨é ç·¨è­¯çš„ CHAPTER_REGEX_STRONG
        keyword_regex = rf"{search_keyword}"


        # ğŸŒŸ æ ¸å¿ƒä¿®æ­£é»ï¼šæ ¹æ“š offset (å¼·åˆ¶é ä½ç§»é‡) å‹•æ…‹èª¿æ•´æœå°‹ç¯„åœ
        # range_to_search æ±ºå®šäº†æœå°‹ç¯„åœçš„çµ•å°å€¼å¤§å° (ä¾‹å¦‚ offset=3, range_to_search=4, æœå°‹ +/-3)
        range_to_search = max(3, offset + 1)

        # å»ºç«‹ä¸‰å€‹å­åˆ—è¡¨ï¼š[0], [1, 2, 3...], [-1, -2, -3...]
        delta_zero = [0]
        delta_positive = list(range(1, range_to_search)) # [1, 2, 3, ...]
        # è² å‘åˆ—è¡¨éœ€è¦åè½‰ï¼Œä»¥ç¢ºä¿ -3, -2, -1 çš„é †åº
        delta_negative = list(range(-range_to_search + 1, 0)) # [-3, -2, -1]

        # æŒ‰ç…§æ‚¨çš„è¦æ±‚é †åºæ‹¼æ¥ï¼š[0] + [æ­£å‘] + [åè½‰çš„è² å‘]
        # ç¢ºä¿è² å‘æ˜¯å¾æœ€å¤§è² æ•¸åˆ°æœ€å°è² æ•¸
        # ä¾‹å¦‚ï¼šoffset=3, range_to_search=4ï¼Œ delta_negative=[-3, -2, -1]
        search_deltas = delta_zero + delta_positive + delta_negative


        print(f"\nã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘åœ¨é æ¸¬é ç¢¼ {candidate_start} å‘¨åœ (Â±{range_to_search-1}) åŸ·è¡Œå¾®èª¿æœå°‹...")
        print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘ä½¿ç”¨é—œéµå­—/æ¨¡å¼åµæ¸¬: StrongRegex='ç¬¬Xç« ', Keyword='{core_keyword}'")

        print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘æœå°‹é †åº: {candidate_start} é  + {search_deltas} é ä½ç§»")

        for delta in search_deltas:
            candidate_idx = candidate_start + delta - 1 # è½‰æˆ 0-based ç´¢å¼•
            candidate_page_num = candidate_idx + 1 # 1-based é ç¢¼

            if 0 <= candidate_idx < max_pages:

                blocks = doc[candidate_idx].get_text("blocks")
                page_text_raw = "\n".join([b[4] for b in blocks])
                page_text_for_search = page_text_raw.upper().replace(' ', '') # ç§»é™¤ç©ºæ ¼å¾Œè½‰æ›å¤§å¯«ä»¥åˆ©é—œéµå­—æœå°‹

                match = None
                is_keyword_match = False

                # å„ªå…ˆä½¿ç”¨å¼·æ¨¡å¼ï¼ˆç¬¬Xç« ï¼‰
                match = CHAPTER_REGEX_STRONG.search(page_text_raw)

                # å¦‚æœå¼·æ¨¡å¼æœªåŒ¹é…ï¼Œä½¿ç”¨é—œéµå­—æ¨¡ç³ŠåŒ¹é…
                if not match and len(core_keyword) >= 3:

                    # é€²è¡Œæ¨¡ç³ŠåŒ¹é…ï¼šå°‹æ‰¾é—œéµå­—
                    if core_keyword.upper().replace(' ', '') in page_text_for_search:
                         is_keyword_match = True
                         # pass # è®“ is_keyword_match = True é€²å…¥ if match: å€å¡Š (è¦‹ä¸‹æ–¹ä¿®æ­£)


                if match or is_keyword_match:

                    # æ±ºå®šä½ç§»
                    new_actual_page = candidate_page_num
                    old_current_page = all_chapters[0]["page"]
                    shift_delta = new_actual_page - old_current_page

                    if match:
                        # å¾ Strong Regex åŒ¹é…ä¸­æ“·å–æ•´è¡Œ
                        start_pos = match.start()
                        line_start = page_text_raw.rfind('\n', 0, start_pos)
                        if line_start == -1: line_start = 0
                        line_end = page_text_raw.find('\n', start_pos)
                        if line_end == -1: line_end = len(page_text_raw)
                        detected_line = page_text_raw[line_start:line_end].strip()
                        detection_mode = "Strong Regex"
                    else: # is_keyword_match = True
                        # å¾æ•´å€‹é é¢æ–‡å­—ä¸­ï¼Œæ‰¾åˆ°é—œéµå­—ä¸¦æ“·å–å…¶æ‰€åœ¨çš„è¡Œ
                        keyword_index_in_raw = page_text_raw.upper().find(core_keyword.upper())

                        if keyword_index_in_raw != -1:
                            line_start = page_text_raw.rfind('\n', 0, keyword_index_in_raw)
                            if line_start == -1: line_start = 0
                            line_end = page_text_raw.find('\n', keyword_index_in_raw)
                            if line_end == -1: line_end = len(page_text_raw)
                            detected_line = page_text_raw[line_start:line_end].strip()
                            detection_mode = f"Keyword Match: {core_keyword}"
                        else:
                             # é—œéµå­—æœå°‹å¤±æ•—ï¼Œå¯èƒ½ç™¼ç”Ÿåœ¨ç©ºæ ¼ç§»é™¤ç­‰æ“ä½œå¾Œ
                             print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘... é ç¢¼ {candidate_page_num}ï¼šé—œéµå­—åŒ¹é…å¾Œç„¡æ³•æ“·å–ä¸Šä¸‹æ–‡ï¼Œè·³éã€‚")
                             continue

                        # ä¿®æ­£ï¼šå¦‚æœåµæ¸¬åˆ°çš„è¡Œä»ç„¶æ˜¯ç©ºçš„ï¼Œå‰‡è·³éï¼Œç¹¼çºŒå°‹æ‰¾
                    if not detected_line:
                        print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘... é ç¢¼ {candidate_page_num}ï¼šåµæ¸¬æ¨¡å¼:{detection_mode}ï¼Œä½†å…§å®¹ç‚ºç©ºï¼Œè·³éã€‚")
                        continue

                    print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘âœ… åœ¨**ç¬¬ {candidate_page_num} é **åµæ¸¬åˆ°æ¨™é¡Œã€‚")
                    print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘åµæ¸¬æ¨¡å¼: **{detection_mode}**")
                    print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘åµæ¸¬åˆ°çš„**å®Œæ•´æ¨™é¡Œè¡Œ**: **'{detected_line}'**")

                    if shift_delta != 0:
                        print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘åŸ·è¡Œå¾®èª¿æ ¡æ­£ä½ç§»: **{shift_delta}** é åˆ°æ‰€æœ‰ç« ç¯€ã€‚")
                        for chap in all_chapters:
                            chap["page"] += shift_delta
                    else:
                        print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘âœ… å¾®èª¿ç²¾ç¢ºã€‚é¦–ç« é ç¢¼: {new_actual_page} (ç„¡é¡å¤–ä½ç§»)ã€‚")

                    found_valid_title = True
                    break # æ‰¾åˆ°å¾Œç«‹å³é€€å‡º delta è¿´åœˆ
                else:
                    print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘... é ç¢¼ {candidate_page_num}ï¼šæœªæ‰¾åˆ°ç« ç¯€æ¨™é¡Œã€‚")

        if not found_valid_title:
              # range_to_search-1 æ˜¯å¯¦éš›ä½ç§»çš„æœ€å¤§çµ•å°å€¼ (ä¾‹å¦‚ 4-1 = 3)
              print(f"ã€é ç¢¼æ ¡æ­£-éšæ®µ2ã€‘âŒ å„˜ç®¡æª¢æŸ¥äº† {candidate_start} é å‘¨åœ Â±{range_to_search-1} é ï¼Œä»æ‰¾ä¸åˆ°æ˜ç¢ºç« ç¯€æ¨™é¡Œã€‚ä¸é€²è¡Œå¾®èª¿æ ¡æ­£ã€‚")

    elif source != "toc":
        print(f"ã€é ç¢¼æ ¡æ­£ã€‘Source={source} (é TOC æ¨¡å¼)ï¼Œè·³éé ç¢¼ä½ç§»æ ¡æ­£ã€‚")
    else:
        print("ã€é ç¢¼æ ¡æ­£ã€‘ç„¡æœ‰æ•ˆç« ç¯€æ•¸æ“šï¼Œè·³éé ç¢¼ä½ç§»æ ¡æ­£ã€‚")

    all_chapters.sort(key=lambda x: x['page'])
    print("--- é ç¢¼ä½ç§»è£œæ­£é‚è¼¯çµæŸ ---")


    # === æ­¥é©Ÿ 3: é‡å°åŒé ç« ç¯€ï¼ŒæŒ‰ Y åº§æ¨™æ’åº (ä½¿ç”¨å‚³å…¥çš„ doc) ===
    chapters_by_page = {}; final_sorted_chapters = []

    original_order_map = {chap['title']: i for i, chap in enumerate(all_chapters)}

    for chap in all_chapters:
        page_idx = chap["page"] - 1
        if page_idx not in chapters_by_page: chapters_by_page[page_idx] = []
        chapters_by_page[page_idx].append(chap)

    for page_idx in sorted(chapters_by_page.keys()):
        page_chapters = chapters_by_page[page_idx]

        if len(page_chapters) > 1:

            if source == "outline":
                sorted_page_chapters = sorted(page_chapters, key=lambda x: original_order_map[x['title']])
                final_sorted_chapters.extend(sorted_page_chapters)
                continue

            page = doc[page_idx]
            blocks = page.get_text("blocks")
            y_coords_map = {}
            for chap in page_chapters:
                y_coord = find_y_coord_for_title(page, chap["title"], blocks)
                if y_coord is not None:
                    y_coords_map[chap["title"]] = (y_coord, chap)
                else:
                    # å°‡æ‰¾ä¸åˆ°åº§æ¨™çš„æ¨™é¡Œç§»åˆ°é é¢åº•éƒ¨ (ä¾‹å¦‚é å°¾å…è²¬è²æ˜)
                    y_coords_map[chap["title"]] = (float('inf') if chap["title"].lower().strip() == 'disclaimer' else float('-inf'), chap)

            sorted_by_y = sorted(y_coords_map.values(), key=lambda x: x[0])
            sorted_page_chapters = [chap_info for y_coord, chap_info in sorted_by_y if y_coord != float('inf')]
            final_sorted_chapters.extend(sorted_page_chapters)
        else:
            final_sorted_chapters.extend(page_chapters)

    all_chapters = final_sorted_chapters

    # === æ­¥é©Ÿ 4: å»ºç«‹æœ€çµ‚ Index (ä½¿ç”¨å‚³å…¥çš„ doc) ===
    for i, chap in enumerate(all_chapters):
        title = chap["title"]
        start_page = chap["page"]

        if i + 1 < len(all_chapters):
            next_chap = all_chapters[i + 1]
            end_page = next_chap["page"]
        else:
            end_page = total_pages + 1

        if end_page < start_page:
            end_page = start_page

        safe_title = UNSAFE_FILENAME_CHARS_PATTERN.sub('', title).strip()
        safe_title = create_safe_filename(safe_title, max_len=50)

        base_name = safe_title

        temp_info = {
            "title": title,
            "page_start": start_page,
            "page_end": end_page,
            "out_file": f"{base_name}.txt",
            "out_dir": f"{base_name}",
            "text_len": 0,
            "tables_count": 0,
        }
        results_index.append(temp_info)

    # å·²åœç”¨è¼¸å‡º chapters_index.json
    # final_output_index = [
    #     {"title": r['title'], "page_start": r['page_start'], "page_end": r['page_end']}
    #     for r in results_index
    # ]
    # index_only_dir = "chapters_index_only"
    # os.makedirs(index_only_dir, exist_ok=True)
    # idx_file = os.path.join(index_only_dir, INDEX_FILENAME)
    #
    # with open(idx_file, "w", encoding="utf-8") as f:
    #     json.dump(final_output_index, f, ensure_ascii=False, indent=2)
    # print(f"\nğŸ“„ å·²è¼¸å‡ºå„ªåŒ–å¾Œçš„ç« ç¯€ç´¢å¼•æª” ({INDEX_FILENAME}) â†’ {idx_file}")

    return results_index

def process_chapters_batch(doc, chapters_index_list, all_camelot_tables, base_out_dir=OUT_CHAPTERS_DIR, dpi=200, debug_plot=False, use_multithread=True):
    """
    æ ¹æ“šç« ç¯€ç´¢å¼•åˆ—è¡¨è™•ç†æ¯ä¸€ç« ç¯€çš„å…§å®¹ã€‚
    å¤šåŸ·è¡Œç·’ï¼šä½¿ç”¨ ThreadPoolExecutor å¹³è¡Œè™•ç†é é¢

    åƒæ•¸ï¼š
        use_multithread: æ˜¯å¦å•Ÿç”¨å¤šåŸ·è¡Œç·’è™•ç†ï¼ˆé è¨­ Trueï¼‰
    """
    os.makedirs(base_out_dir, exist_ok=True)

    total_pdf_pages = len(doc)

    # æ•ˆèƒ½å„ªåŒ–ï¼šé å…ˆå»ºç«‹è¡¨æ ¼çš„é ç¢¼ç´¢å¼•ï¼Œé¿å…æ¯é éƒ½éæ­·æ‰€æœ‰è¡¨æ ¼
    # æ™‚é–“è¤‡é›œåº¦å¾ O(ç¸½é æ•¸ Ã— ç¸½è¡¨æ ¼æ•¸) é™ç‚º O(ç¸½è¡¨æ ¼æ•¸ + ç¸½é æ•¸)
    tables_by_page = {}
    for t in all_camelot_tables:
        page_num = t.page
        if page_num not in tables_by_page:
            tables_by_page[page_num] = []
        tables_by_page[page_num].append(t)

    # å–å¾—æœ€ä½³åŸ·è¡Œç·’æ•¸
    max_workers = get_optimal_workers() if use_multithread else 1

    print(f"\n==========================================")
    print(f"ğŸ”„ é–‹å§‹è™•ç† {len(chapters_index_list)} å€‹ç« ç¯€... ğŸ“„ æ–‡ä»¶ç¸½é æ•¸: {total_pdf_pages}")
    if use_multithread:
        print(f"ğŸš€ å¤šåŸ·è¡Œç·’æ¨¡å¼ï¼š{max_workers} å€‹åŸ·è¡Œç·’")
    else:
        print(f"ğŸ“Œ å–®åŸ·è¡Œç·’æ¨¡å¼")
    print(f"==========================================")

    updated_index_list = []

    for i, chap_info in enumerate(chapters_index_list):
        # æ¯å€‹ç« ç¯€å»ºç«‹ç¨ç«‹çš„åœ–ç‰‡å¿«å–ï¼Œç« ç¯€å…§å»é‡
        # ä½¿ç”¨ threading.Lock ä¿è­·å¿«å–ï¼Œé¿å…å¤šåŸ·è¡Œç·’ç«¶çˆ­
        chapter_image_cache = {}
        cache_lock = threading.Lock()  # ä¿è­·å¿«å–çš„é–

        title = chap_info["title"]
        start_page = chap_info["page_start"]
        end_page = chap_info["page_end"]
        out_file_name = chap_info["out_file"]
        assets_dir_name = chap_info["out_dir"]

        actual_end_page = min(end_page, total_pdf_pages + 1) # +1 æ˜¯ç‚ºäº†è™•ç†æœ€å¾Œä¸€ç« 

        chapter_safe_name = assets_dir_name

        print(f"\n--- ğŸ“š è™•ç†ç« ç¯€ {i + 1}/{len(chapters_index_list)}: **{title}** (é ç¢¼: {start_page} - {actual_end_page - 1 if actual_end_page > total_pdf_pages else actual_end_page -1}) ---")

        # 1. ç« ç¯€è³‡ç”¢è³‡æ–™å¤¾è·¯å¾‘
        chapter_assets_dir = os.path.join(base_out_dir, assets_dir_name)
        os.makedirs(chapter_assets_dir, exist_ok=True)

        # 2. TXT æª”è·¯å¾‘
        out_path = os.path.join(chapter_assets_dir, out_file_name)

        all_chapter_items = []
        current_text_len = 0
        current_tables_count = 0

        start_y_coord = None
        try:
            page = doc[start_page - 1]
            blocks = page.get_text("blocks")
            start_y_coord = find_y_coord_for_title(page, title, blocks)
        except Exception as e:
            pass

        end_y_coord = None
        if i + 1 < len(chapters_index_list):
            next_chap = chapters_index_list[i + 1]
            # ä¸‹ä¸€ç« ç¯€çš„é–‹å§‹é ç¢¼èˆ‡æœ¬ç« ç¯€çš„çµæŸé ç¢¼ç›¸åŒ
            if next_chap["page_start"] == (end_page):
                try:
                    page = doc[next_chap["page_start"] - 1]
                    blocks = page.get_text("blocks")
                    end_y_coord = find_y_coord_for_title(page, next_chap["title"], blocks)
                except Exception as e:
                    pass


        # å¤šåŸ·è¡Œç·’è™•ç†é é¢
        if use_multithread and (end_page - start_page) > 2:
            # æº–å‚™é é¢è™•ç†ä»»å‹™
            def process_page_task(page_num):
                """å–®é è™•ç†ä»»å‹™ï¼ˆç”¨æ–¼å¤šåŸ·è¡Œç·’ï¼‰"""
                try:
                    page = doc[page_num]
                    page_height = page.rect.height

                    is_start_page_in_chapter = (page_num == start_page - 1)
                    is_last_page_in_chapter = (page_num == end_page - 1)

                    y_start_to_process = start_y_coord if is_start_page_in_chapter and start_y_coord is not None else None
                    y_end_to_process = None

                    if is_last_page_in_chapter and end_y_coord is not None and i + 1 < len(chapters_index_list):
                        next_chap = chapters_index_list[i + 1]
                        if next_chap["page_start"] == (page_num + 1):
                            y_end_to_process = min(end_y_coord, page_height)
                    elif is_last_page_in_chapter and end_page == total_pdf_pages + 1:
                        y_end_to_process = None

                    # æ•ˆèƒ½å„ªåŒ–ï¼šä½¿ç”¨é å…ˆå»ºç«‹çš„ç´¢å¼•ç›´æ¥æŸ¥è©¢ç•¶å‰é çš„è¡¨æ ¼ï¼ˆO(1) æŸ¥è©¢ï¼‰
                    current_page_tables = tables_by_page.get(page_num + 1, [])

                    items = process_single_page_and_get_items(
                        page, page_height, chapter_assets_dir, chapter_safe_name,
                        start_y_coordinate=y_start_to_process,
                        end_y_coordinate=y_end_to_process,
                        dpi=dpi, debug_draw=debug_plot,
                        camelot_tables=current_page_tables,
                        chapter_image_cache=chapter_image_cache,
                        cache_lock=cache_lock
                    )
                    return (page_num, items)
                except Exception as e:
                    print(f"âš ï¸ è™•ç†é é¢ {page_num + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    return (page_num, [])

            # ä½¿ç”¨å¤šåŸ·è¡Œç·’è™•ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰é é¢ä»»å‹™
                future_to_page = {
                    executor.submit(process_page_task, page_num): page_num
                    for page_num in range(start_page - 1, end_page)
                    if page_num < total_pdf_pages
                }

                # æ”¶é›†çµæœ
                for future in as_completed(future_to_page):
                    page_num, items = future.result()
                    all_chapter_items.extend(items)

            # æŒ‰é ç¢¼å’Œ Y åº§æ¨™æ’åºï¼ˆç¢ºä¿é †åºæ­£ç¢ºï¼‰
            all_chapter_items.sort(key=lambda x: (x['page_num'], x['y_center']))

        else:
            # å–®åŸ·è¡Œç·’è™•ç†ï¼ˆåŸå§‹é‚è¼¯ï¼‰
            for page_num in range(start_page - 1, end_page):
                if page_num >= total_pdf_pages:
                    break

                page = doc[page_num]
                page_height = page.rect.height

                is_start_page_in_chapter = (page_num == start_page - 1)
                is_last_page_in_chapter = (page_num == end_page - 1)

                y_start_to_process = start_y_coord if is_start_page_in_chapter and start_y_coord is not None else None
                y_end_to_process = None

                if is_last_page_in_chapter and end_y_coord is not None and i + 1 < len(chapters_index_list):
                    next_chap = chapters_index_list[i + 1]
                    if next_chap["page_start"] == (page_num + 1):
                        # å¦‚æœä¸‹ä¸€ç« çš„é–‹å§‹é èˆ‡æœ¬ç« çµæŸé ç¢¼ç›¸åŒï¼Œå‰‡æœ¬é çš„çµæŸ y åº§æ¨™ç‚ºä¸‹ä¸€ç« æ¨™é¡Œçš„ y åº§æ¨™
                        y_end_to_process = min(end_y_coord, page_height)
                elif is_last_page_in_chapter and end_page == total_pdf_pages + 1:
                    # å¦‚æœæ˜¯æœ€å¾Œä¸€ç« çš„æœ€å¾Œä¸€é ï¼Œè™•ç†åˆ°é å°¾
                    y_end_to_process = None

                # æ•ˆèƒ½å„ªåŒ–ï¼šä½¿ç”¨é å…ˆå»ºç«‹çš„ç´¢å¼•ç›´æ¥æŸ¥è©¢ç•¶å‰é çš„è¡¨æ ¼ï¼ˆO(1) æŸ¥è©¢ï¼‰
                current_page_tables = tables_by_page.get(page_num + 1, [])

                items = process_single_page_and_get_items(
                    page, page_height, chapter_assets_dir, chapter_safe_name,
                    start_y_coordinate=y_start_to_process,
                    end_y_coordinate=y_end_to_process,
                    dpi=dpi, debug_draw=debug_plot,
                    camelot_tables=current_page_tables,
                    chapter_image_cache=chapter_image_cache,
                    cache_lock=cache_lock
                )
                all_chapter_items.extend(items)

        # === å¯«å…¥ TXT ä¸¦å­˜è‡³å‘é‡è³‡æ–™åº« ===
        
        chunks = []
        with open(out_path, "w", encoding="utf-8-sig") as f_out:
            for item in sorted(all_chapter_items, key=lambda x: (x['page_num'], x['y_center'])):
                chunk_text = None
                if item["type"] == "text":
                    chunk_text = item["content"]
                    prefix = f"## {title} - " if item.get("is_title") else ""
                    f_out.write(prefix + item["content"] + "\n")
                    current_text_len += len(item["content"])
                elif item["type"] == "table":
                    chunk_text = item["content"].to_csv(index=False, sep="\t")
                    current_tables_count += 1
                    f_out.write(f"\n[TABLE {item['name']} - Page {item['page_num']}]\n")
                    item["content"].to_csv(f_out, index=False, sep="\t", encoding="utf-8-sig")
                    f_out.write(f"[END TABLE]\n")
                elif item["type"] == "figure":
                    figure_title = f"æ¨™é¡Œ: {item['title']}" if item.get('title') and not item['title'].startswith('[é‡è¤‡åœ–å½¢') else "ç„¡æ¨™é¡Œ"

                    relative_path_name = os.path.basename(item['content'])

                    f_out.write(f"\n[FIGURE {relative_path_name} - Page {item['page_num']}]\n")
                    f_out.write(f"åœ–ç‰‡è·¯å¾‘: {relative_path_name}\n")
                    f_out.write(f"{figure_title}\n")
                    f_out.write(f"[END FIGURE]\n")
                    
                if chunk_text:
                    ic_model = extract_ic_model(chunk_text) or title
                    page = str(item.get("page_num"))
                    section = extract_section(chunk_text) or title
                    
                    chunks.append({
                        "text": chunk_text,
                        "ic_model": ic_model,
                        "page": page,
                        "section": section
                    })


        # === æ›´æ–°ç« ç¯€ç´¢å¼• (æš«å­˜è³‡è¨Š) ===
        chap_info["text_len"] = current_text_len
        chap_info["tables_count"] = current_tables_count
        updated_index_list.append(chap_info)

        print(f"âœ… å®Œæˆç« ç¯€ï¼š{title}ï¼Œæ–‡å­—é•·åº¦: {current_text_len}ï¼Œè¡¨æ ¼æ•¸: {current_tables_count}, å…± {len(chunks)} å€‹ chunks, é–‹å§‹é€ AI ...")
        asyncio.run(ingest_chunks(chunks, title))

    # å·²åœç”¨è¼¸å‡º chapters_index.json åˆ° structured_chapters_final
    # final_output_index = [
    #     {"title": r['title'], "page_start": r['page_start'], "page_end": r['page_end']}
    #     for r in updated_index_list
    # ]
    #
    # idx_file = os.path.join(base_out_dir, INDEX_FILENAME)
    # with open(idx_file, "w", encoding="utf-8") as f:
    #     json.dump(final_output_index, f, ensure_ascii=False, indent=2)
    # print(f"\nğŸ“„ å·²è¼¸å‡ºæœ€çµ‚ç« ç¯€ç´¢å¼•æª” (title/page_start/page_end) â†’ {idx_file}")

    return updated_index_list


# === ä¸»ç¨‹å¼é€²å…¥é» (å„ªåŒ–å¾Œ) ===
if __name__ == "__main__":
    pdf_file = "dummy.pdf"

    try:
        # --- æ­¥é©Ÿ 1: ä¸Šå‚³æª”æ¡ˆèˆ‡åˆå§‹åŒ– ---
        is_mock = isinstance(files, MockFiles)

        if is_mock:
            # æœ¬åœ°ç’°å¢ƒï¼šä½¿ç”¨æª”æ¡ˆé¸æ“‡å°è©±æ¡†
            try:
                from tkinter import Tk, filedialog
                print("è«‹é¸æ“‡ PDF æª”æ¡ˆ...")
                root = Tk()
                root.withdraw()  # éš±è—ä¸»è¦–çª—
                root.attributes('-topmost', True)  # è®“å°è©±æ¡†ç½®é ‚
                pdf_file = filedialog.askopenfilename(
                    title="é¸æ“‡ PDF æª”æ¡ˆ",
                    filetypes=[("PDF files", "*.pdf"), ("All files", "*.*")]
                )
                root.destroy()

                if not pdf_file:
                    print("âŒ æœªé¸æ“‡æª”æ¡ˆï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
                    exit()
                print(f"âœ… å·²é¸æ“‡ï¼š{pdf_file}")
            except ImportError:
                print("âŒ ç„¡æ³•è¼‰å…¥æª”æ¡ˆé¸æ“‡å°è©±æ¡† (éœ€è¦ tkinter)ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
                exit()
        else:
            # Colab ç’°å¢ƒï¼šä½¿ç”¨åŸæœ¬çš„ä¸Šå‚³æ–¹å¼
            print("è«‹ä¸Šå‚³ PDF æª”æ¡ˆï¼š")
            uploaded = files.upload()

            if not uploaded:
                print("âŒ æ²’æœ‰æª”æ¡ˆä¸Šå‚³ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
                exit()

            pdf_file = next(iter(uploaded))
            print(f"âœ… å·²ä¸Šå‚³ï¼š{pdf_file}")

        # â±ï¸ é–‹å§‹è¨ˆæ™‚
        program_start_time = time.time()
        print(f"\nâ±ï¸ é–‹å§‹è™•ç†... (è¨ˆæ™‚é–‹å§‹)")

        # ğŸš€ æ•ˆèƒ½å„ªåŒ–é» A: åƒ…åœ¨æ­¤è™•æ‰“é–‹ä¸€æ¬¡ PDF æ–‡ä»¶
        with fitz.open(pdf_file) as doc:
            total_pages = len(doc)
            print(f"é€™ä»½ PDF ç¸½å…±æœ‰ {total_pages} é ã€‚")

            # --- æ­¥é©Ÿ 2: å»ºç«‹ç« ç¯€ç´¢å¼• ---
            catalog_json_file = "catalog.json"

            # ä½¿ç”¨å·²é–‹å•Ÿçš„ doc ç‰©ä»¶
            outline_chapters = find_outline(doc)
            catalog_data = []
            source = ""

            raw_chapters_list = []

            if outline_chapters:
                print(f"\nâœ… å¾ Outline (æ›¸ç±¤) åµæ¸¬åˆ° {len(outline_chapters)} å€‹ç« ç¯€ã€‚")
                raw_chapters_list = outline_chapters
                catalog_data = [{"catalog_page": 0, "content": "\n".join([f"{c['title']} ... {c['page']}" for c in outline_chapters])}]
                source = "outline"
            else:
                # ä½¿ç”¨å·²é–‹å•Ÿçš„ doc ç‰©ä»¶
                catalog_pages = find_catalog_pages(doc)
                if not catalog_pages:
                    print("\nâš ï¸ æ²’æ‰¾åˆ° Outline æˆ–ç›®éŒ„é ï¼Œç„¡æ³•å»ºç«‹ç« ç¯€ç´¢å¼•ï¼Œç¨‹å¼çµæŸã€‚")
                    exit()
                for page_idx, content in catalog_pages:
                    catalog_data.append({"catalog_page": page_idx + 1, "content": content})
                    raw_chapters_list.extend(parse_catalog_text(content))

                if source == "toc":
                    raw_chapters_list.sort(key=lambda x: x['page'])

                print(f"\nâœ… å·²æ‰¾åˆ° {len(catalog_pages)} é ç›®éŒ„ã€‚")
                source = "toc"

            # å·²åœç”¨è¼¸å‡º chapters_raw_outline.json
            # with open(RAW_OUTLINE_FILENAME, "w", encoding="utf-8") as f:
            #     raw_output = [{"title": r['title'], "page": r['page']} for r in raw_chapters_list]
            #     json.dump(raw_output, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ“„ å·²è¼¸å‡º**åŸå§‹**ç« ç¯€é †åºæª” (ç„¡ä»»ä½•æ ¡æ­£) â†’ **{RAW_OUTLINE_FILENAME}**")

            # å·²åœç”¨è¼¸å‡º catalog.json
            # with open(catalog_json_file, "w", encoding="utf-8") as f:
            #     json.dump(catalog_data, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ“„ å·²è¼¸å‡ºåŸå§‹ç›®éŒ„è³‡æ–™æª” (åŒ…å«åŸå§‹æ–‡å­—) â†’ {catalog_json_file}")

            # å‚³å…¥ doc ç‰©ä»¶
            # é€™è£¡æœƒè¼¸å‡ºé ç¢¼æ ¡æ­£çš„ print ä¿¡æ¯
            chapters_index_list = build_chapters_index(doc, catalog_data, raw_chapters_list, source=source)

            if not chapters_index_list:
                print("âŒ ç„¡æ³•å¾ç›®éŒ„æˆ–å¤§ç¶±è§£æå‡ºæœ‰æ•ˆçš„ç« ç¯€åˆ—è¡¨ï¼Œç¨‹å¼çµæŸã€‚")
                exit()

            # ğŸš€ æ•ˆèƒ½å„ªåŒ–é» B: æå‰åŸ·è¡Œè¡¨æ ¼åµæ¸¬ï¼ˆåƒ…è™•ç†ç« ç¯€ç¯„åœå…§çš„é é¢ï¼‰
            # å¾ç« ç¯€ç´¢å¼•ä¸­æå–æ‰€æœ‰æ¶µè“‹çš„é ç¢¼
            chapter_pages = set()
            for chap in chapters_index_list:
                for page_num in range(chap['page_start'], chap['page_end']):
                    if page_num <= total_pages:
                        chapter_pages.add(page_num)

            # å°‡é ç¢¼åˆ—è¡¨è½‰æ›ç‚º Camelot æ¥å—çš„ç¯„åœå­—ä¸²æ ¼å¼
            sorted_pages = sorted(chapter_pages)
            if sorted_pages:
                ranges = []
                start = sorted_pages[0]
                end = sorted_pages[0]

                for p in sorted_pages[1:]:
                    if p == end + 1:
                        end = p
                    else:
                        ranges.append(f"{start}-{end}" if start != end else f"{start}")
                        start = end = p

                ranges.append(f"{start}-{end}" if start != end else f"{start}")
                pages_str = ','.join(ranges)
            else:
                pages_str = "all"

            print(f"\nğŸ” æ­£åœ¨åŸ·è¡Œè¡¨æ ¼åµæ¸¬ï¼ˆåƒ…è™•ç†ç« ç¯€é é¢: {len(chapter_pages)}/{total_pages} é ï¼‰...")
            print(f"   é ç¢¼ç¯„åœ: {pages_str}")
            all_camelot_tables = camelot.read_pdf(pdf_file, pages=pages_str, flavor="lattice")
            print(f"âœ… è¡¨æ ¼åµæ¸¬å®Œæˆï¼Œå…±åµæ¸¬åˆ° {len(all_camelot_tables)} å€‹è¡¨æ ¼ã€‚")


            # --- æ­¥é©Ÿ 3: æ‰¹æ¬¡è™•ç†ç« ç¯€å…§å®¹ ---
            # å‚³å…¥ doc ç‰©ä»¶ å’Œ all_camelot_tables
            process_chapters_batch(
                doc,
                chapters_index_list,
                all_camelot_tables,
                debug_plot=False,  # ğŸ’¡ æ³¨æ„ï¼šå¦‚æœè¦å•Ÿç”¨ Debug ç¹ªåœ–ï¼Œè«‹å°‡ debug_plot=False æ”¹ç‚º debug_plot=True
                use_multithread=True  # ğŸ’¡ å•Ÿç”¨å¤šåŸ·è¡Œç·’åŠ é€Ÿï¼ˆè¨­ç‚º False å¯é—œé–‰ï¼‰
            )

        # PDF æ–‡ä»¶åœ¨æ­¤è™•é—œé–‰ (when fitz.open(pdf_file) scope ends)

        # â±ï¸ è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
        program_end_time = time.time()
        total_program_time = program_end_time - program_start_time

        print("\n" + "=" * 60)
        print("ğŸ‰ ç¨‹å¼åŸ·è¡Œå®Œæˆï¼")
        print("=" * 60)
        print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {total_program_time:.2f} ç§’")
        print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {total_program_time/60:.2f} åˆ†é˜")
        if total_program_time >= 3600:
            print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {total_program_time/3600:.2f} å°æ™‚")
        print("=" * 60)

    except ImportError as e:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„å‡½å¼åº«ï¼š{e}ã€‚è«‹ç¢ºèªæ‰€æœ‰å¿…è¦çš„å¥—ä»¶ (**PyMuPDF**, **camelot-py**, **opencv-python**, **pandas**, **numpy**) çš†å·²å®‰è£ã€‚")

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
